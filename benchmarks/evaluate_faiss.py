import numpy as np
from typing import Optional, Tuple
import numpy as np
import faiss
import mlflow
import os
import logging
import platform, socket, psutil
import argparse


ENVIRONMENT_INFO = {
    "load_before_experiment": os.getloadavg()[2],
    "platform": platform.platform(),
    "platform_version": platform.version(),
    "platform_release": platform.release(),
    "architecture": platform.machine(),
    "processor": platform.processor(),
    "hostname": socket.gethostname(),
    "ram_gb": round(psutil.virtual_memory().total / (1024.0**3)),
    "num_cores": psutil.cpu_count(logical=True),
}


DATASETS = {
    "mnist-784-euclidean": (
        "mnist-784-euclidean.train.npy",
        "mnist-784-euclideean.test.npy",
        "mnist-784-euclidean.gtruth.npy",
    ),
    "glove-25-angular": (
        "glove-25-angular.train.npy",
        "glove-25-angular.test.npy",
        "glove-25-angular.gtruth.npy",
    ),
    "glove-100-angular": (
        "glove-100-angular.train.npy",
        "glove-100-angular.test.npy",
        "glove-100-angular.gtruth.npy",
    ),
    "glove-200-angular": (
        "glove-200-angular.train.npy",
        "glove-200-angular.test.npy",
        "glove-200-angular.gtruth.npy",
    ),
    "sift-128-euclidean": (
        "sift-128-euclidean.train.npy",
        "sift-128-euclidean.test.npy",
        "sift-128-euclidean.gtruth.npy",
    ),
}


def _log_environment_info():
    for key, val in ENVIRONMENT_INFO.items():
        mlflow.log_param(key, val)


def setup_mlflow_auth(username, password):
    os.environ["MLFLOW_TRACKING_USERNAME"] = username
    os.environ["MLFLOW_TRACKING_PASSWORD"] = password


def set_tracking_uri(uri):
    mlflow.set_tracking_uri(uri=uri)


def log_mlflow_run(
    dataset: Optional[str] = None,
    run_name: Optional[str] = None,
    algorithm: Optional[str] = None,
    querying_time: Optional[float] = None,
    num_training_queries: Optional[float] = None,
    num_test_queries: Optional[float] = None,
    recall: Optional[float] = None,
    indexing_time: Optional[float] = None,
):
    with mlflow.start_run(
        run_name=run_name, tags={"dataset": dataset, "algorithm": algorithm}
    ):
        _log_environment_info()
        mlflow.log_param("indexing_time", indexing_time)
        mlflow.log_param("querying_time", querying_time)
        mlflow.log_param("indexed_query_count", num_training_queries)
        mlflow.log_param("num_queries", num_test_queries)

        if num_test_queries is not None and querying_time is not None:
            mlflow.log_metric("queries_per_second", num_test_queries / querying_time)

        mlflow.log_metric("recall", recall)


def load_benchmark_dataset(dataset_name: str | None) -> Tuple[np.ndarray]:
    """
    This assumes that we have a /data/<dataset_name> already present.
    This data directory can be generated by running
        $ /bin/download_anns_datasets.sh <dataset-name>

    This directory will be expected to have the following files:
        - <dataset-name>/<dataset-name>.train.npy
        - <dataset-name>/<dataset-name>.test.npy
        - <dataset-name>/<dataset-name>.gtruth.npy
    """
    dataset_name = dataset_name.lower()
    if not dataset_name in DATASETS.keys():
        raise AssertionError(
            f"{dataset_name=} not in the list of supported datasets."
            "Consider adding it to the list of checking if you misspelled the name."
        )

    train_file, queries_file, gtruth_file = DATASETS[dataset_name]

    return np.load(train_file), np.load(queries_file), np.load(gtruth_file)


def compute_recall(index, queries: np.ndarray, ground_truth: np.ndarray, k=100):
    """
    Compute recall for given queries, ground truth, and a Faiss index.

    Parameters:
    - index: The Faiss index to search.
    - queries: The query vectors.
    - ground_truth: The ground truth indices for each query.
    - k: Number of neighbors to search.

    Returns:
    - Mean recall over all queries.
    """
    _, top_k_indices = index.search(queries, k)

    # Convert each ground truth list to a set for faster lookup
    ground_truth_sets = [set(gt) for gt in ground_truth]

    mean_recall = 0

    for idx, k_neighbors in enumerate(top_k_indices):
        query_recall = sum(
            1 for neighbor in k_neighbors if neighbor in ground_truth_sets[idx]
        )
        mean_recall += query_recall / k

    recall = mean_recall / len(queries)
    logging.info(f"Recall@{k}: {recall}")
    return recall


def train_hnsw_index(data, serialize=True):
    # configure the index
    d = data.shape[1]  # data dimension
    m = 8  # number of subquantizers
    nbits = 8  # bits per subvector index

    # Create the HNSW quantizer
    quantizer = faiss.IndexHNSWFlat(d, 16)  # 16 is a typical value for HNSW M parameter
    quantizer.hnsw.efConstruction = 128  # Typical values are between 20-64
    quantizer.hnsw.efSearch = 128  # Typical values are between 20-64

    # Create the IVFPQ index
    index = faiss.IndexIVFPQ(quantizer, d, 100, m, nbits)

    # Train the index
    print("Training index...")
    index.train(data)

    # Add vectors to the index
    index.add(data)

    if serialize:
        # Serialize the index to disk
        buffer = faiss.serialize_index(index)
        index_size = len(buffer)

        print(f"Index size: {index_size} bytes")

    return index


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    parser = argparse.ArgumentParser(description="")
    parser.add_argument(
        "--dataset", required=True, help="ANNS benchmark dataset to run on."
    )
    parser.add_arugment("--log_metrics", required=False, help="Log metrics to MLFlow.")

    args = parser.parse_args()
    train_data, queries, ground_truth = load_benchmark_dataset(
        dataset_name=args.dataset
    )

    index = train_hnsw_index(data=train_data)
    recall = compute_recall(index=index, queries=queries, ground_truth=ground_truth)

    if args.log_metrics:
        log_mlflow_run(
            dataset=args.dataset,
            run_name="faiss-benchmark",
            algorithm="faiss.IndexHNSW",
            num_training_queries=train_data.shape[0],
            num_test_queries=queries.shape[0],
            recall=recall,
        )
