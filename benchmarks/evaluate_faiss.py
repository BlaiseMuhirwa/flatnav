import numpy as np
from typing import Optional, Tuple
import numpy as np
import faiss
import mlflow
import os
import logging
import platform, socket, psutil
import argparse


ENVIRONMENT_INFO = {
    "load_before_experiment": os.getloadavg()[2],
    "platform": platform.platform(),
    "platform_version": platform.version(),
    "platform_release": platform.release(),
    "architecture": platform.machine(),
    "processor": platform.processor(),
    "hostname": socket.gethostname(),
    "ram_gb": round(psutil.virtual_memory().total / (1024.0**3)),
    "num_cores": psutil.cpu_count(logical=True),
}


DATASETS = {
    "mnist-784-euclidean": (
        "mnist-784-euclidean.train.npy",
        "mnist-784-euclidean.test.npy",
        "mnist-784-euclidean.gtruth.npy",
    ),
    "glove-25-angular": (
        "glove-25-angular.train.npy",
        "glove-25-angular.test.npy",
        "glove-25-angular.gtruth.npy",
    ),
    "glove-100-angular": (
        "glove-100-angular.train.npy",
        "glove-100-angular.test.npy",
        "glove-100-angular.gtruth.npy",
    ),
    "glove-200-angular": (
        "glove-200-angular.train.npy",
        "glove-200-angular.test.npy",
        "glove-200-angular.gtruth.npy",
    ),
    "sift-128-euclidean": (
        "sift-128-euclidean.train.npy",
        "sift-128-euclidean.test.npy",
        "sift-128-euclidean.gtruth.npy",
    ),
}


def _log_environment_info():
    for key, val in ENVIRONMENT_INFO.items():
        mlflow.log_param(key, val)


def set_tracking_uri():
    uri = os.environ.get("MLFLOW_TRACKING_URI", None)
    if not uri:
        raise RuntimeError("MLFlow tracking URI not set.")
    mlflow.set_tracking_uri(uri=uri)


def log_mlflow_run(
    dataset: Optional[str] = None,
    run_name: Optional[str] = None,
    algorithm: Optional[str] = None,
    querying_time: Optional[float] = None,
    num_training_queries: Optional[float] = None,
    num_test_queries: Optional[float] = None,
    recall: Optional[float] = None,
    indexing_time: Optional[float] = None,
):
    with mlflow.start_run(
        run_name=run_name, tags={"dataset": dataset, "algorithm": algorithm}
    ):
        _log_environment_info()
        mlflow.log_param("indexing_time", indexing_time)
        mlflow.log_param("querying_time", querying_time)
        mlflow.log_param("indexed_query_count", num_training_queries)
        mlflow.log_param("num_queries", num_test_queries)

        if num_test_queries is not None and querying_time is not None:
            mlflow.log_metric("queries_per_second", num_test_queries / querying_time)

        mlflow.log_metric("recall", recall)


def load_benchmark_dataset(dataset_name: str | None) -> Tuple[np.ndarray]:
    """
    This assumes that we have a /data/<dataset_name> already present.
    This data directory can be generated by running
        $ /bin/download_anns_datasets.sh <dataset-name>

    This directory will be expected to have the following files:
        - <dataset-name>/<dataset-name>.train.npy
        - <dataset-name>/<dataset-name>.test.npy
        - <dataset-name>/<dataset-name>.gtruth.npy
    """
    dataset_name = dataset_name.lower()
    if not dataset_name in DATASETS.keys():
        raise AssertionError(
            f"{dataset_name=} not in the list of supported datasets."
            "Consider adding it to the list of checking if you misspelled the name."
        )

    train_file, queries_file, gtruth_file = DATASETS[dataset_name]
    base_dir = os.path.join(os.getcwd(), "..", "data", dataset_name)

    return (
        np.load(os.path.join(base_dir, train_file)),
        np.load(os.path.join(base_dir, queries_file)),
        np.load(os.path.join(base_dir, gtruth_file)),
    )


def compute_recall(index, queries: np.ndarray, ground_truth: np.ndarray, k=100):
    """
    Compute recall for given queries, ground truth, and a Faiss index.

    Args:
        - index: The Faiss index to search.
        - queries: The query vectors.
        - ground_truth: The ground truth indices for each query.
        - k: Number of neighbors to search.

    Returns:
        Mean recall over all queries.
    """
    _, top_k_indices = index.search(queries, k)

    # Convert each ground truth list to a set for faster lookup
    ground_truth_sets = [set(gt) for gt in ground_truth]

    mean_recall = 0

    for idx, k_neighbors in enumerate(top_k_indices):
        query_recall = sum(
            1 for neighbor in k_neighbors if neighbor in ground_truth_sets[idx]
        )
        mean_recall += query_recall / k

    recall = mean_recall / len(queries)
    return recall


def train_hnsw_index(
    data,
    pq_m,
    num_node_links,
    ef_construction: Optional[int] = 128,
    ef_search: Optional[int] = 128,
    serialize=False,
):
    """
    Train a HNSW index topped with product quantization
    Args:
        - pq_m: Number of subquantizers for PQ. This should exactly divide the
            dataset dimensions.
        - num_node_links: Maximum number of links to keep for each node in the graph
        - serialize: Serialize so we can get a sense of how large the index binary is.

    Returns:
        Index

    Helpful link on correct usage: https://github.com/facebookresearch/faiss/issues/1621
    """
    # configure the index
    dim = data.shape[1]  # data dimension

    # Create the HNSW index
    index = faiss.IndexHNSWPQ(dim, pq_m, num_node_links)
    index.hnsw.efConstruction = ef_construction
    index.hnsw.efSearchh = ef_search

    logging.info("Training index...")
    index.train(data)

    # Add vectors to the index
    index.add(data)

    if serialize:
        # Serialize the index to disk
        buffer = faiss.serialize_index(index)
        index_size = len(buffer)

        logging.info(f"Index size: {index_size} bytes")

    return index


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    parser = argparse.ArgumentParser(description="")
    parser.add_argument(
        "--dataset", required=True, help="ANNS benchmark dataset to run on."
    )
    parser.add_argument("--log_metrics", required=False, help="Log metrics to MLFlow.")

    args = parser.parse_args()
    train_data, queries, ground_truth = load_benchmark_dataset(
        dataset_name=args.dataset
    )

    ef_constructions = [64, 128, 256]
    ef_searches = [32, 64, 128]
    num_node_links = [8, 16, 32, 64]

    for m in num_node_links:
        for ef_construction in ef_constructions:
            for ef_search in ef_searches:
                index = train_hnsw_index(
                    data=train_data,
                    pq_m=8,
                    num_node_links=m,
                    ef_construction=ef_construction,
                    ef_search=ef_search,
                )
                recall = compute_recall(
                    index=index, queries=queries, ground_truth=ground_truth
                )

                logging.info(
                    f"Recall@100: {recall}, node_links={m}, ef_cons={ef_construction}, ef_search={ef_search}"
                )

                if args.log_metrics:
                    set_tracking_uri()
                    log_mlflow_run(
                        dataset=args.dataset,
                        run_name="faiss-benchmark",
                        algorithm="faiss.IndexHNSW",
                        num_training_queries=train_data.shape[0],
                        num_test_queries=queries.shape[0],
                        recall=recall,
                    )
